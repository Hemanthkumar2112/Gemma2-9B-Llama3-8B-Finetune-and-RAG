{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\nif major_version >= 8:\n    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\nelse:\n    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n    !pip install --no-deps xformers trl peft accelerate bitsandbytes\npass\n!pip install triton transformers\n!pip install -U datasets\n!pip install --pre -U xformers ##### this take some time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Note** Restart the Kernal after package installation**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:48:17.720366Z","iopub.execute_input":"2024-04-19T15:48:17.720756Z","iopub.status.idle":"2024-04-19T15:48:17.850227Z","shell.execute_reply.started":"2024-04-19T15:48:17.720713Z","shell.execute_reply":"2024-04-19T15:48:17.849442Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom IPython.display import display_markdown\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/llama-3-8b-bnb-4bit\",  \n]  #### loadin llama 3 model in 4 bit to fine tune","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:48:17.854873Z","iopub.execute_input":"2024-04-19T15:48:17.855133Z","iopub.status.idle":"2024-04-19T15:48:23.926428Z","shell.execute_reply.started":"2024-04-19T15:48:17.855109Z","shell.execute_reply":"2024-04-19T15:48:23.925520Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:48:23.928329Z","iopub.execute_input":"2024-04-19T15:48:23.928721Z","iopub.status.idle":"2024-04-19T15:49:25.364770Z","shell.execute_reply.started":"2024-04-19T15:48:23.928694Z","shell.execute_reply":"2024-04-19T15:49:25.363950Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03d46d3e2dc429db11516a2a441a49b"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.4\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.dev778. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f6e6d895704a2995edf608efd74b5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db945957c9544656bac1f416d221d665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b1901e8c5ee45938b902347468ed163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482061b34ea14a26852dc2b866593df1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e111ede2c44a93b172ea36f8a01328"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n2024-04-19 15:49:14.219228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-19 15:49:14.219343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-19 15:49:14.372882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:25.365896Z","iopub.execute_input":"2024-04-19T15:49:25.366485Z","iopub.status.idle":"2024-04-19T15:49:26.216053Z","shell.execute_reply.started":"2024-04-19T15:49:25.366458Z","shell.execute_reply":"2024-04-19T15:49:26.215315Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"#### dataset import \ndataset_name = \"VMware/open-instruct\"\nfrom datasets import load_dataset\ndataset = load_dataset(dataset_name, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:26.218365Z","iopub.execute_input":"2024-04-19T15:49:26.219027Z","iopub.status.idle":"2024-04-19T15:49:29.842078Z","shell.execute_reply.started":"2024-04-19T15:49:26.218991Z","shell.execute_reply":"2024-04-19T15:49:29.841258Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6155a70c38452983a9833a3dd6c61b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/57.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e42af5f383cc4421a0afed0d508c9b2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/142622 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37cefb05e5e94fb8ba71a460f2c57976"}},"metadata":{}}]},{"cell_type":"code","source":"# Must add EOS_TOKEN at response last line\nEOS_TOKEN = tokenizer.eos_token \ndef mapping_response(sample):\n    sample['text'] = sample['alpaca_prompt']+\"\\n\"+sample['response']+EOS_TOKEN\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:29.843164Z","iopub.execute_input":"2024-04-19T15:49:29.843501Z","iopub.status.idle":"2024-04-19T15:49:29.848691Z","shell.execute_reply.started":"2024-04-19T15:49:29.843455Z","shell.execute_reply":"2024-04-19T15:49:29.847682Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"### selecting only 2000 samples for testing\ndataset = dataset.select(range(2000))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:29.849932Z","iopub.execute_input":"2024-04-19T15:49:29.850796Z","iopub.status.idle":"2024-04-19T15:49:30.012477Z","shell.execute_reply.started":"2024-04-19T15:49:29.850761Z","shell.execute_reply":"2024-04-19T15:49:30.011472Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(mapping_response)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:30.013787Z","iopub.execute_input":"2024-04-19T15:49:30.014089Z","iopub.status.idle":"2024-04-19T15:49:30.304663Z","shell.execute_reply.started":"2024-04-19T15:49:30.014065Z","shell.execute_reply":"2024-04-19T15:49:30.303724Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33b7bec750e14ed299762e72457aa789"}},"metadata":{}}]},{"cell_type":"code","source":"def prompt_inference(prmpt):\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n    inputs = tokenizer(\n    [\n        prmpt\n    ], return_tensors = \"pt\").to(\"cuda\")\n\n    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n    return tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1]","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:30.305834Z","iopub.execute_input":"2024-04-19T15:49:30.306120Z","iopub.status.idle":"2024-04-19T15:49:30.311604Z","shell.execute_reply.started":"2024-04-19T15:49:30.306095Z","shell.execute_reply":"2024-04-19T15:49:30.310619Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"prompt = dataset['alpaca_prompt'][90]\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:30.314468Z","iopub.execute_input":"2024-04-19T15:49:30.314742Z","iopub.status.idle":"2024-04-19T15:49:30.333025Z","shell.execute_reply.started":"2024-04-19T15:49:30.314712Z","shell.execute_reply":"2024-04-19T15:49:30.332094Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate the lyrics to a song that begins like this \"Hey hey let's go...\"\n\n### Response:\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"result\")\ndisplay_markdown(prompt_inference(prmpt=prompt),raw=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:49:30.333994Z","iopub.execute_input":"2024-04-19T15:49:30.334282Z","iopub.status.idle":"2024-04-19T15:50:05.417591Z","shell.execute_reply.started":"2024-04-19T15:49:30.334259Z","shell.execute_reply":"2024-04-19T15:50:05.416601Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"result\n","output_type":"stream"},{"output_type":"display_data","data":{"text/markdown":" \nHey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey hey let's go, hey"},"metadata":{}}]},{"cell_type":"code","source":"### the above result is not clear ,without fine tuning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\", ### taking text column from the dataset\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 2,\n        warmup_steps = 500,\n        max_steps = 800,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 100,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",num_train_epochs =1\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:50:05.419034Z","iopub.execute_input":"2024-04-19T15:50:05.419725Z","iopub.status.idle":"2024-04-19T15:50:08.474165Z","shell.execute_reply.started":"2024-04-19T15:50:05.419688Z","shell.execute_reply":"2024-04-19T15:50:08.473312Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0389487b8bd543c785c5b36624c7b958"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T15:53:11.098740Z","iopub.execute_input":"2024-04-19T15:53:11.099147Z","iopub.status.idle":"2024-04-19T16:21:11.419893Z","shell.execute_reply.started":"2024-04-19T15:53:11.099114Z","shell.execute_reply":"2024-04-19T16:21:11.419032Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 2,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 2\n\\        /    Total batch size = 2 | Total steps = 800\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112882922225254, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4f74e4792814be3ad92da8d51df2ffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240419_155342-66nqx8jp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hemanth2112/huggingface/runs/66nqx8jp' target=\"_blank\">sage-energy-80</a></strong> to <a href='https://wandb.ai/hemanth2112/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hemanth2112/huggingface' target=\"_blank\">https://wandb.ai/hemanth2112/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hemanth2112/huggingface/runs/66nqx8jp' target=\"_blank\">https://wandb.ai/hemanth2112/huggingface/runs/66nqx8jp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 27:04, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.475300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.231300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.176200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.221700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.197700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.234700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.197600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.211500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=800, training_loss=1.2432494068145752, metrics={'train_runtime': 1679.8268, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.476, 'total_flos': 2.051551084444877e+16, 'train_loss': 1.2432494068145752, 'epoch': 0.8})"},"metadata":{}}]},{"cell_type":"code","source":"prompt = dataset['alpaca_prompt'][92]\nprint(prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"result\")\ndisplay_markdown(prompt_inference(prmpt=prompt),raw=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T16:21:18.927271Z","iopub.execute_input":"2024-04-19T16:21:18.928002Z","iopub.status.idle":"2024-04-19T16:21:26.213153Z","shell.execute_reply.started":"2024-04-19T16:21:18.927961Z","shell.execute_reply":"2024-04-19T16:21:26.211922Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"result\n","output_type":"stream"},{"output_type":"display_data","data":{"text/markdown":" \nHey hey let's go\nWe're gonna have a good time\nWe're gonna make some noise\nWe're gonna have a blast\nHey hey let's go\nWe're gonna sing and dance\nWe're gonna make some memories\nWe're gonna have a blast\nHey hey let's go\nWe're gonna have a party\nWe're gonna make some friends\nWe're gonna have a blast\nHey hey let's go\nWe're gonna have a good time\nWe're gonna make some noise\nWe're gonna have a blast<|end_of_text|>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}