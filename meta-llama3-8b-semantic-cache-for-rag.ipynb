{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8161765,"sourceType":"datasetVersion","datasetId":4828888}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.38.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  \"trl==0.7.11\" \\\n  \"peft==0.8.2\" \\\n    \"langchain\" \\\n\"sentence-transformers\" \\\n\"faiss-cpu\"\n!pip install unstructured\n!pip install pdfminer\n!pip install pdfminer.six\n!pip install -U langchain-community==0.2.4","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T05:56:37.128836Z","iopub.execute_input":"2024-06-08T05:56:37.129664Z","iopub.status.idle":"2024-06-08T05:59:04.390168Z","shell.execute_reply.started":"2024-06-08T05:56:37.129630Z","shell.execute_reply":"2024-06-08T05:59:04.389221Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (2024.3.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.59.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.32.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nCollecting transformers==4.38.2\n  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting datasets==2.16.1\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nCollecting accelerate==0.26.1\n  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\nCollecting evaluate==0.4.1\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting bitsandbytes==0.42.0\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nCollecting trl==0.7.11\n  Downloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\nCollecting peft==0.8.2\n  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\nCollecting langchain\n  Downloading langchain-0.2.3-py3-none-any.whl.metadata (6.9 kB)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (2.32.3)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (4.66.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1) (2.1.2)\nCollecting responses<0.19 (from evaluate==0.4.1)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.11.4)\nCollecting tyro>=0.5.11 (from trl==0.7.11)\n  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_core-0.2.5-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.75-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\nCollecting packaging>=20.0 (from transformers==4.38.2)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.2)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.11)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.1)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (0.1.2)\nDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.7.11-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-0.2.3-py3-none-any.whl (974 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\nDownloading langsmith-0.1.75-py3-none-any.whl (124 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, packaging, orjson, fsspec, faiss-cpu, dill, responses, multiprocess, bitsandbytes, tyro, tokenizers, langsmith, accelerate, transformers, langchain-core, datasets, trl, sentence-transformers, peft, langchain-text-splitters, evaluate, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.1\n    Uninstalling fsspec-2024.3.1:\n      Successfully uninstalled fsspec-2024.3.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.26.1 bitsandbytes-0.42.0 datasets-2.16.1 dill-0.3.7 evaluate-0.4.1 faiss-cpu-1.8.0 fsspec-2023.10.0 langchain-0.2.3 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langsmith-0.1.75 multiprocess-0.70.15 orjson-3.10.3 packaging-23.2 peft-0.8.2 responses-0.18.0 sentence-transformers-3.0.1 shtab-1.7.1 tokenizers-0.15.2 transformers-4.38.2 trl-0.7.11 tyro-0.8.4\nCollecting unstructured\n  Downloading unstructured-0.14.5-py3-none-any.whl.metadata (28 kB)\nCollecting chardet (from unstructured)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting filetype (from unstructured)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting python-magic (from unstructured)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured) (5.2.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from unstructured) (3.2.4)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured) (0.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from unstructured) (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured) (4.12.2)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from unstructured) (2.12.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from unstructured) (0.6.6)\nCollecting python-iso639 (from unstructured)\n  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\nCollecting langdetect (from unstructured)\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from unstructured) (1.26.4)\nCollecting rapidfuzz (from unstructured)\n  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: backoff in /opt/conda/lib/python3.10/site-packages (from unstructured) (2.2.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from unstructured) (4.9.0)\nCollecting unstructured-client (from unstructured)\n  Downloading unstructured_client-0.23.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from unstructured) (1.14.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.5)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->unstructured) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->unstructured) (0.9.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect->unstructured) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->unstructured) (2024.2.2)\nRequirement already satisfied: deepdiff>=6.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (7.0.1)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (0.27.0)\nCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: mypy-extensions>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.0.0)\nCollecting nest-asyncio>=1.6.0 (from unstructured-client->unstructured)\n  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: packaging>=23.1 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (23.2)\nRequirement already satisfied: pypdf>=4.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (4.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\nCollecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.0)\nDownloading unstructured-0.14.5-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured_client-0.23.2-py3-none-any.whl (39 kB)\nDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\nDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=04fe92a2e7ef7ed48f3e0e87f6a6680dcdd3239a1ab2fc1eb3874fdd3e71b7d3\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: filetype, rapidfuzz, python-magic, python-iso639, nest-asyncio, langdetect, jsonpath-python, chardet, requests-toolbelt, unstructured-client, unstructured\n  Attempting uninstall: nest-asyncio\n    Found existing installation: nest-asyncio 1.5.8\n    Uninstalling nest-asyncio-1.5.8:\n      Successfully uninstalled nest-asyncio-1.5.8\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-5.2.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 nest-asyncio-1.5.8 python-iso639-2024.4.27 python-magic-0.4.27 rapidfuzz-3.9.3 requests-toolbelt-1.0.0 unstructured-0.14.5 unstructured-client-0.23.2\nCollecting pdfminer\n  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from pdfminer) (3.20.0)\nBuilding wheels for collected packages: pdfminer\n  Building wheel for pdfminer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140086 sha256=9174cd9a9af7921348b2291e7ead1f385bee28c545e07d3b5125c9cc765ba2ba\n  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\nSuccessfully built pdfminer\n\u001b[33mWARNING: Error parsing requirements for nest-asyncio: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/nest_asyncio-1.5.8.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: pdfminer\nSuccessfully installed pdfminer-20191125\nCollecting pdfminer.six\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six) (41.0.7)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\nDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for nest-asyncio: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/nest_asyncio-1.5.8.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: pdfminer.six\nSuccessfully installed pdfminer.six-20231228\nCollecting langchain-community\n  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.6)\nRequirement already satisfied: langchain<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.2.3)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.2.5)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.1.75)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.5.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.9.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nDownloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for nest-asyncio: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/nest_asyncio-1.5.8.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: langchain-community\nSuccessfully installed langchain-community-0.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T06:05:27.554143Z","iopub.execute_input":"2024-06-08T06:05:27.554545Z","iopub.status.idle":"2024-06-08T06:05:27.710248Z","shell.execute_reply.started":"2024-06-08T06:05:27.554513Z","shell.execute_reply":"2024-06-08T06:05:27.709331Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom IPython.display import display_markdown\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom transformers import pipeline\nimport transformers\nimport time\nfrom langchain.document_loaders import UnstructuredPDFLoader,PDFMinerLoader,TextLoader\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n \n\n# Hugging Face model id\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n \n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,token=hf_token,\n    model_kwargs={\n        \"torch_dtype\": torch.float16,\n        \"quantization_config\": {\"load_in_4bit\": True},\n        \"low_cpu_mem_usage\": True,\n    },\n)\n\nterminators =  [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\n### for semantic cache\nvector_store = FAISS()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T06:05:33.772336Z","iopub.execute_input":"2024-06-08T06:05:33.773222Z","iopub.status.idle":"2024-06-08T06:08:05.281733Z","shell.execute_reply.started":"2024-06-08T06:05:33.773185Z","shell.execute_reply":"2024-06-08T06:08:05.280950Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-06-08 06:05:40.505038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-08 06:05:40.505154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-08 06:05:40.616144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8d538869a740728848877d6111e3f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9039d3289c741b395c32fd544bec082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a0b452aca60460fb431cb91587a774b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0064663ea0b54980be20803011e82587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f564ceb1c67453cadd23fa3068c16ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be5ff51e79249c6998779d62aa6c89c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba640b9d3dda4cd7a48a53cd94b8616d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fec0222826314b6e84000969934c0709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0a7df56598492b96af36ed1f22c860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ca5e144bbd4bd4b1227422f33f0cec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85493e4570f2464c8f49140365da4c73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18cd5cd237c4c6e8d3efed23d8568ae"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.vectorstores import FAISS\nfrom langchain.docstore import InMemoryDocstore\nimport faiss\nembeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n# Initialize an empty FAISS index\ndimension = embeddings.client.get_sentence_embedding_dimension()\nindex = faiss.IndexFlatL2(dimension)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:53:10.988630Z","iopub.execute_input":"2024-06-08T07:53:10.988993Z","iopub.status.idle":"2024-06-08T07:53:12.143522Z","shell.execute_reply.started":"2024-06-08T07:53:10.988966Z","shell.execute_reply":"2024-06-08T07:53:12.142556Z"},"trusted":true},"execution_count":222,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"docstore = InMemoryDocstore()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:53:12.598045Z","iopub.execute_input":"2024-06-08T07:53:12.598378Z","iopub.status.idle":"2024-06-08T07:53:12.602535Z","shell.execute_reply.started":"2024-06-08T07:53:12.598351Z","shell.execute_reply":"2024-06-08T07:53:12.601447Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"vector_store = FAISS(\n    embedding_function=embeddings,\n    index=index,\n    docstore=docstore,\n    index_to_docstore_id={}\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:53:13.763654Z","iopub.execute_input":"2024-06-08T07:53:13.764018Z","iopub.status.idle":"2024-06-08T07:53:13.768622Z","shell.execute_reply.started":"2024-06-08T07:53:13.763990Z","shell.execute_reply":"2024-06-08T07:53:13.767646Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"### Pdf file Path for RAG\npdf_file_path = \"/kaggle/input/deep-learning-ian-goodfellow/DeepLearningBook.pdf\"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T06:50:20.101859Z","iopub.execute_input":"2024-06-08T06:50:20.102250Z","iopub.status.idle":"2024-06-08T06:50:20.107218Z","shell.execute_reply.started":"2024-06-08T06:50:20.102219Z","shell.execute_reply":"2024-06-08T06:50:20.106155Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"### this class used to retrieve the text from pdf and chunk it \nclass Langchain_RAG:\n    def __init__(self, pdf_file_path):\n        self.embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n        self.pdf_file_path = pdf_file_path\n        print(\"Loading PDF file, this may take time to process...\")\n        self.loader = PDFMinerLoader(self.pdf_file_path)\n        self.data = self.loader.load()\n        print(\"PDF file loaded.\")\n        print(\"Chunking...\")\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n        self.texts = text_splitter.split_documents(self.data)\n        print(\"Chunking completed.\")\n        self.get_vec_value = FAISS.from_documents(self.texts, self.embeddings)\n        print(\"Vector values saved.\")\n        self.retriever = self.get_vec_value.as_retriever(search_kwargs={\"k\": 4})\n\n    def __call__(self, query):\n        relevant_docs = self.retriever.get_relevant_documents(query)\n        return \"\".join([doc.page_content for doc in relevant_docs])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:47:11.604413Z","iopub.execute_input":"2024-06-08T07:47:11.604813Z","iopub.status.idle":"2024-06-08T07:47:11.613535Z","shell.execute_reply.started":"2024-06-08T07:47:11.604784Z","shell.execute_reply":"2024-06-08T07:47:11.612598Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"import time\n\n# This class is used to generate responses from an LLM model\nclass Llama3_8B_gen:\n    def __init__(self, pipeline, embeddings, vector_store, threshold):\n        self.pipeline = pipeline\n        self.embeddings = embeddings\n        self.vector_store = vector_store\n        self.threshold = threshold\n        \n    @staticmethod\n    def generate_prompt(query,retrieved_text):\n        messages = [\n            {\"role\": \"system\", \"content\": \"Answer the Question for the Given below context and information and not prior knowledge, only give the output result \\n\\ncontext:\\n\\n{}\".format(retrieved_text) },\n            {\"role\": \"user\", \"content\": query},]\n        return pipeline.tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)\n    \n    def semantic_cache(self, query, prompt):\n        query_embedding = self.embeddings.embed_documents([query])\n        similar_docs = self.vector_store.similarity_search_with_score_by_vector(query_embedding[0], k=1)\n        \n        if similar_docs and similar_docs[0][1] <self.threshold:\n            self.print_bold_underline(\"---->> From Cache\")\n            return similar_docs[0][0].metadata['response']\n        else:\n            self.print_bold_underline(\"---->> From LLM\")\n            output = self.pipeline(prompt, max_new_tokens=512, eos_token_id=terminators, do_sample=True, temperature=0.7, top_p=0.9)\n            \n            response = output[0][\"generated_text\"][len(prompt):]\n            self.vector_store.add_texts(texts = [query], \n                       metadatas = [{'response': response},])\n            \n            return response\n            \n    def generate(self, query, retrieved_context):\n        start_time = time.time()\n        \n        prompt = self.generate_prompt(query, retrieved_context)\n        res = self.semantic_cache(query, prompt)   \n        \n        end_time = time.time()\n        execution_time = end_time - start_time\n        self.print_bold_underline(f\"LLM generated in {execution_time:.6f} seconds\")\n        \n        return res\n\n    @staticmethod\n    def print_bold_underline(text):\n        print(f\"\\033[1m\\033[4m{text}\\033[0m\")\n\n \n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:54:08.179962Z","iopub.execute_input":"2024-06-08T07:54:08.180821Z","iopub.status.idle":"2024-06-08T07:54:08.193133Z","shell.execute_reply.started":"2024-06-08T07:54:08.180785Z","shell.execute_reply":"2024-06-08T07:54:08.192033Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"text_gen = Llama3_8B_gen(pipeline=pipeline,embeddings=embeddings,\n                         vector_store=vector_store,threshold=0.1)\nretriever = Langchain_RAG(pdf_file_path=pdf_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:54:09.143417Z","iopub.execute_input":"2024-06-08T07:54:09.144338Z","iopub.status.idle":"2024-06-08T07:54:09.148608Z","shell.execute_reply.started":"2024-06-08T07:54:09.144306Z","shell.execute_reply":"2024-06-08T07:54:09.147551Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"def Rag_qa(query):\n    retriever_context = retriever(query)\n    result = text_gen.generate(query,retriever_context)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:53:24.693637Z","iopub.execute_input":"2024-06-08T07:53:24.694408Z","iopub.status.idle":"2024-06-08T07:53:24.698506Z","shell.execute_reply.started":"2024-06-08T07:53:24.694381Z","shell.execute_reply":"2024-06-08T07:53:24.697639Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"code","source":"Rag_qa(\"What is Deep learning ?\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:53:25.338987Z","iopub.execute_input":"2024-06-08T07:53:25.339875Z","iopub.status.idle":"2024-06-08T07:53:31.716090Z","shell.execute_reply.started":"2024-06-08T07:53:25.339827Z","shell.execute_reply":"2024-06-08T07:53:31.715229Z"},"trusted":true},"execution_count":227,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[4m---->> From LLM\u001b[0m\n\u001b[1m\u001b[4mLLM generated in 6.349944 seconds\u001b[0m\n","output_type":"stream"},{"execution_count":227,"output_type":"execute_result","data":{"text/plain":"'According to the given context, Deep Learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain, statistics and applied math. Specifically, it is a type of machine learning that allows computer systems to improve with experience and data.'"},"metadata":{}}]},{"cell_type":"code","source":"Rag_qa(\"What is Deep learning ?\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:54:12.878255Z","iopub.execute_input":"2024-06-08T07:54:12.879003Z","iopub.status.idle":"2024-06-08T07:54:12.917337Z","shell.execute_reply.started":"2024-06-08T07:54:12.878970Z","shell.execute_reply":"2024-06-08T07:54:12.916303Z"},"trusted":true},"execution_count":231,"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[4m---->> From Cache\u001b[0m\n\u001b[1m\u001b[4mLLM generated in 0.014279 seconds\u001b[0m\n","output_type":"stream"},{"execution_count":231,"output_type":"execute_result","data":{"text/plain":"'According to the given context, Deep Learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain, statistics and applied math. Specifically, it is a type of machine learning that allows computer systems to improve with experience and data.'"},"metadata":{}}]},{"cell_type":"code","source":"Rag_qa(\"Explain back propagation algorithm.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:54:59.619034Z","iopub.execute_input":"2024-06-08T07:54:59.619746Z","iopub.status.idle":"2024-06-08T07:55:39.649059Z","shell.execute_reply.started":"2024-06-08T07:54:59.619714Z","shell.execute_reply":"2024-06-08T07:55:39.648177Z"},"trusted":true},"execution_count":232,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[4m---->> From LLM\u001b[0m\n\u001b[1m\u001b[4mLLM generated in 40.006118 seconds\u001b[0m\n","output_type":"stream"},{"execution_count":232,"output_type":"execute_result","data":{"text/plain":"\"The back-propagation algorithm is a strategy for efficiently computing the gradient of the loss function with respect to the model's parameters in a neural network. It's a key component of the training process for neural networks.\\n\\nThe algorithm works by performing a forward pass through the network, computing the output for a given input, and then performing a backward pass, computing the gradient of the loss function with respect to the model's parameters.\\n\\nHere's a step-by-step breakdown of the back-propagation algorithm:\\n\\n1. Forward Pass: The algorithm starts by computing the output of the network for a given input. This is done by propagating the input through the network, layer by layer, using the network's weights and biases.\\n2. Loss Computation: The output of the network is compared to the target output, and the loss function is computed. This is typically done using a mean squared error (MSE) or cross-entropy loss function.\\n3. Backward Pass: The algorithm then performs a backward pass, computing the gradient of the loss function with respect to the model's parameters. This is done by propagating the error backwards through the network, layer by layer, using the chain rule.\\n4. Gradient Computation: The gradient of the loss function with respect to the model's parameters is computed at each layer. This is typically done using the derivative of the activation function at each layer.\\n5. Weight Update: The gradient is then used to update the model's weights and biases using an optimization algorithm, such as stochastic gradient descent (SGD).\\n\\nThe back-propagation algorithm has several advantages, including:\\n\\n* Efficient computation of the gradient: The algorithm can compute the gradient of the loss function with respect to the model's parameters efficiently, even for large neural networks.\\n* Scalability: The algorithm can be parallelized, making it suitable for large-scale neural network training.\\n* Flexibility: The algorithm can be used with different activation functions, loss functions, and optimization algorithms.\\n\\nHowever, the back-propagation algorithm also has some limitations, including:\\n\\n* Computational complexity: The algorithm can be computationally expensive, especially for large neural networks.\\n* Overfitting: The algorithm can lead to overfitting if the model is too complex or the training data is too small.\\n\\nOverall, the back-propagation algorithm is a powerful tool for training neural networks and has been widely used in many applications, including computer vision, natural language processing, and speech recognition.\""},"metadata":{}}]},{"cell_type":"code","source":"Rag_qa(\"back propagation algorithm.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:56:08.089179Z","iopub.execute_input":"2024-06-08T07:56:08.089913Z","iopub.status.idle":"2024-06-08T07:56:34.927914Z","shell.execute_reply.started":"2024-06-08T07:56:08.089877Z","shell.execute_reply":"2024-06-08T07:56:34.926845Z"},"trusted":true},"execution_count":234,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[4m---->> From LLM\u001b[0m\n\u001b[1m\u001b[4mLLM generated in 26.812080 seconds\u001b[0m\n","output_type":"stream"},{"execution_count":234,"output_type":"execute_result","data":{"text/plain":"'The back-propagation algorithm is used to compute the gradient of the loss function with respect to the parameters of a neural network. The algorithm involves two passes:\\n\\n1. Forward propagation: This involves propagating the input through the network to compute the output.\\n2. Backward propagation: This involves propagating the error backward through the network to compute the gradient of the loss function with respect to the parameters.\\n\\nThe algorithm is as follows:\\n\\nAlgorithm 6.3:\\n\\n```\\nRequire: x, the input to the network\\nRequire: y, the target output\\nRequire: ˆy, the output of the neural network\\n\\n1. Forward Propagation:\\n    ˆy = f(x, θ)\\n    where f is the neural network and θ are the parameters\\n\\n2. Compute the loss:\\n    L = (ˆy - y)^2\\n\\n3. Backward Propagation:\\n    Compute the gradient of the loss with respect to the output:\\n        ∂L/∂ˆy = -2*(ˆy - y)\\n\\n    Compute the gradient of the output with respect to the hidden states:\\n        ∂ˆy/∂h = ∂f/∂h\\n\\n    Compute the gradient of the hidden states with respect to the parameters:\\n        ∂h/∂θ =...\\n\\n    Update the parameters:\\n        θ -= α * ∂L/∂θ\\n        where α is the learning rate\\n```\\n\\nNote that this is a simplified version of the algorithm and may vary depending on the specific neural network architecture and implementation.'"},"metadata":{}}]},{"cell_type":"code","source":"Rag_qa(\"back propagation algorithm.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T07:58:29.189302Z","iopub.execute_input":"2024-06-08T07:58:29.189933Z","iopub.status.idle":"2024-06-08T07:58:29.229545Z","shell.execute_reply.started":"2024-06-08T07:58:29.189901Z","shell.execute_reply":"2024-06-08T07:58:29.228017Z"},"trusted":true},"execution_count":235,"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[4m---->> From Cache\u001b[0m\n\u001b[1m\u001b[4mLLM generated in 0.013826 seconds\u001b[0m\n","output_type":"stream"},{"execution_count":235,"output_type":"execute_result","data":{"text/plain":"'The back-propagation algorithm is used to compute the gradient of the loss function with respect to the parameters of a neural network. The algorithm involves two passes:\\n\\n1. Forward propagation: This involves propagating the input through the network to compute the output.\\n2. Backward propagation: This involves propagating the error backward through the network to compute the gradient of the loss function with respect to the parameters.\\n\\nThe algorithm is as follows:\\n\\nAlgorithm 6.3:\\n\\n```\\nRequire: x, the input to the network\\nRequire: y, the target output\\nRequire: ˆy, the output of the neural network\\n\\n1. Forward Propagation:\\n    ˆy = f(x, θ)\\n    where f is the neural network and θ are the parameters\\n\\n2. Compute the loss:\\n    L = (ˆy - y)^2\\n\\n3. Backward Propagation:\\n    Compute the gradient of the loss with respect to the output:\\n        ∂L/∂ˆy = -2*(ˆy - y)\\n\\n    Compute the gradient of the output with respect to the hidden states:\\n        ∂ˆy/∂h = ∂f/∂h\\n\\n    Compute the gradient of the hidden states with respect to the parameters:\\n        ∂h/∂θ =...\\n\\n    Update the parameters:\\n        θ -= α * ∂L/∂θ\\n        where α is the learning rate\\n```\\n\\nNote that this is a simplified version of the algorithm and may vary depending on the specific neural network architecture and implementation.'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Explanation\nWhen generating text directly from the Large Language Model (LLM), the process may take over 40 seconds. However, by caching the generated text, subsequent requests for the same text experience significantly reduced response times. This caching mechanism stores previously generated text, allowing for quick retrieval without the need to regenerate it, thus improving response times for repetitive requests. By leveraging this cache, the system optimizes performance and enhances user experience by minimizing wait times for text generation.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}